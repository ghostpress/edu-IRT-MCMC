{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482bee61-e533-46ba-aa8b-bfa8ccf96607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.chdir(\"/projectnb/labci/Lucia/edu-bayesian-MCMC/\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "from pymc import PolyaGamma as PG\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "from src.samplers.MetropolisGibbs import gibbs\n",
    "from src.samplers.PolyaGamma import polyagamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7fa6ab-4d60-4310-b163-cde69a56e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "I = 100   # exam items\n",
    "P = 1000  # pupils\n",
    "\n",
    "init_a = torch.zeros(I)  \n",
    "init_b = torch.zeros(I) \n",
    "init_t = torch.zeros(P) \n",
    "init_W = torch.zeros(P, I)\n",
    "\n",
    "\n",
    "true_a = torch.empty(size=(I,))      # items' discriminatory power\n",
    "true_b = torch.empty(size=(I,))      # items' difficulty\n",
    "true_theta = torch.empty(size=(P,))  # students' skills\n",
    "true_W = torch.empty(size=(P,I))     # latent variable (for PG case)\n",
    "Y = torch.empty(size=(P, I))         # simulated data\n",
    "\n",
    "# populate a, b:\n",
    "for i in range(I):\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    true_a[i] = U.sample()\n",
    "    true_b[i] = -1.0*U.sample()\n",
    "    \n",
    "# populate theta:\n",
    "for p in range(P):\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    true_theta[p] = U.sample()\n",
    "\n",
    "# populate W:\n",
    "scales = torch.mul(torch.t(true_a[:, None]), true_theta[:, None]) + true_b\n",
    "true_W = torch.from_numpy(pm.draw(PG.dist(h=1, z=scales)))\n",
    "\n",
    "# generate exam data:\n",
    "for i in range(I):\n",
    "    for p in range(P):\n",
    "        prob = torch.exp(true_a[i]*true_theta[p] + true_b[i]) / (1 + torch.exp(true_a[i]*true_theta[p] + true_b[i]))\n",
    "        B = torch.distributions.Bernoulli(prob)\n",
    "        Y[p][i] = B.sample()\n",
    "\n",
    "#Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63a1c29-c84d-4c74-ada3-f82429e7f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs sampler... \n",
      "--------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument std in method wrapper_CUDA_Tensor_Tensor_normal)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m sigmasq_a, sigmasq_b, sigmasq_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# priors\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m samples_mwg \u001b[38;5;241m=\u001b[39m gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, device\u001b[38;5;241m=\u001b[39mdevice, niter\u001b[38;5;241m=\u001b[39mN, adapt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m samples_ada_mwg \u001b[38;5;241m=\u001b[39m gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, device\u001b[38;5;241m=\u001b[39mdevice, niter\u001b[38;5;241m=\u001b[39mN, adapt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/projectnb/labci/Lucia/edu-bayesian-MCMC/src/samplers/MetropolisGibbs.py:271\u001b[0m, in \u001b[0;36mgibbs\u001b[0;34m(init_a, init_b, init_theta, y, sigmasq_a, sigmasq_b, sigmasq_t, device, niter, adapt)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(I):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapt:\n\u001b[0;32m--> 271\u001b[0m         A[s][i], ad \u001b[38;5;241m=\u001b[39m metropolis(device, A[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i], sigmasq_a, avg_acc_a[i], loga_conditional, B[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i], THETA[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y[:,i], sigmasq_a)\n\u001b[1;32m    272\u001b[0m         avg_acc_a[i] \u001b[38;5;241m=\u001b[39m avg_acc_a[i] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39ms)\u001b[38;5;241m*\u001b[39mad\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/labci/Lucia/edu-bayesian-MCMC/src/samplers/MetropolisGibbs.py:131\u001b[0m, in \u001b[0;36mmetropolis\u001b[0;34m(device, prev_sample, sigmasq, avg_acc, full_cond, *full_cond_args)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetropolis\u001b[39m(device, prev_sample, sigmasq, avg_acc, full_cond, \u001b[38;5;241m*\u001b[39mfull_cond_args):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Function to perform random-walk metropolis sampling of a full conditional distribution. \u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        The next state of the random walk\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     prop \u001b[38;5;241m=\u001b[39m prev_sample \u001b[38;5;241m+\u001b[39m (torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m]), sigmasq)\u001b[38;5;241m.\u001b[39msample())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Ensure that values are within range [0,1]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m#if (prop > 1) or (prop < 0):\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m#    A = 0\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m#    logr = full_cond(prop, *full_cond_args) - full_cond(prev_sample, *full_cond_args)    \u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m#    A = torch.min(torch.tensor([1.0]), torch.exp(logr)) \u001b[39;00m\n\u001b[1;32m    139\u001b[0m     logr \u001b[38;5;241m=\u001b[39m full_cond(prop, \u001b[38;5;241m*\u001b[39mfull_cond_args) \u001b[38;5;241m-\u001b[39m full_cond(prev_sample, \u001b[38;5;241m*\u001b[39mfull_cond_args)    \n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/edu-mcmc/lib/python3.11/site-packages/torch/distributions/normal.py:70\u001b[0m, in \u001b[0;36mNormal.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     68\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mexpand(shape), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mexpand(shape))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument std in method wrapper_CUDA_Tensor_Tensor_normal)"
     ]
    }
   ],
   "source": [
    "# Run samplers\n",
    "N = 100\n",
    "sigmasq_a, sigmasq_b, sigmasq_t = torch.tensor([1.0]), torch.tensor([1.0]), torch.tensor([1.0])  # priors\n",
    "\n",
    "samples_mwg = gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N, adapt=False)\n",
    "print(\"\")\n",
    "samples_ada_mwg = gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N, adapt=True)\n",
    "print(\"\")\n",
    "samples_pg = polyagamma(init_a, init_b, init_t, init_W, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4539b3-bf3c-4862-8b1d-85ed4903c11f",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc46fd-14d2-4977-8d6a-3e5b33b0fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9), sharex=True)\n",
    "fig1.suptitle(\"Traceplots for MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "t = range(N)\n",
    "data_to_plot = [samples_mwg[0][:,0], samples_ada_mwg[0][:,0], samples_pg[0][:,0],\n",
    "                samples_mwg[1][:,0], samples_ada_mwg[1][:,0], samples_pg[1][:,0],\n",
    "                samples_mwg[2][:,0], samples_ada_mwg[2][:,0], samples_pg[2][:,0]]\n",
    "\n",
    "true_values = [true_a[0], true_a[0], true_a[0],\n",
    "               true_b[0], true_b[0], true_b[0],\n",
    "               true_theta[0], true_theta[0], true_theta[0]]\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig1.axes):\n",
    "    ax.plot(t, data_to_plot[i])\n",
    "    ax.axhline(true_values[i], color=\"red\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig1.supxlabel('iterations')\n",
    "fig1.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0612b8-4947-4db6-99c5-fe48ab2bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9))\n",
    "fig2.suptitle(\"Histograms of MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig2.axes):\n",
    "    ax.hist(data_to_plot[i], bins=100)\n",
    "    ax.axvline(true_values[i], color=\"red\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ed978-8b85-4a8f-b5a2-0a5720369c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9), sharey=True)\n",
    "fig3.suptitle(\"ACF for MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig3.axes):\n",
    "    tsaplots.plot_acf(data_to_plot[i], lags=50, ax=ax, title=\"\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig3.supxlabel('lag')\n",
    "fig3.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41f25b-c670-4c59-a385-d5eba173998c",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1df740-edc3-4a7f-9b5d-15f4cb028874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(chain, true_val):\n",
    "    MSE = nn.MSELoss()\n",
    "    E = torch.empty(size=(len(chain),))\n",
    "\n",
    "    for s in range(len(chain)):\n",
    "        E[s] = MSE(chain[s], true_val)\n",
    "\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87d0be-334c-43ee-9be7-0d2251e3da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, axs = plt.subplots(nrows=3, ncols=1, figsize=(9,9), sharex=True)\n",
    "fig4.suptitle(\"MSE of MCMC samples of a, b, theta under 3 sampling schemes over time\")\n",
    "\n",
    "t = range(N)\n",
    "errors_to_plot = [[mse(samples_mwg[0], true_a), mse(samples_ada_mwg[0], true_a), mse(samples_pg[0], true_a)],\n",
    "                  [mse(samples_mwg[1], true_b), mse(samples_ada_mwg[1], true_b), mse(samples_pg[1], true_b)],\n",
    "                  [mse(samples_mwg[2], true_theta), mse(samples_ada_mwg[2], true_theta), mse(samples_pg[2], true_theta)]]\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "labels = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a\", \"b\", \"theta\"]\n",
    "\n",
    "for i, ax in enumerate(fig4.axes):\n",
    "    lines = data_to_plot[i]\n",
    "    \n",
    "    for j in range(len(lines)):\n",
    "        ax.plot(t, lines[j], color=colors[j], label=labels[j])\n",
    "\n",
    "for ax, row in zip(axs, rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "axs[0].legend(loc=\"best\")\n",
    "fig4.supxlabel('iterations')\n",
    "fig4.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97e07e-5eb7-42bf-a142-4fbd413c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(errors_to_plot)):\n",
    "    for j in range(len(errors_to_plot[i])):\n",
    "        mean = torch.mean(errors_to_plot[i][j])\n",
    "        print(f\"Overall MSE for variable {rows[i]} under {labels[j]} scheme: {mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874cbd4-f779-48d8-be18-e2d9d49ee8f6",
   "metadata": {},
   "source": [
    "### Asymptotic Variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
