{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482bee61-e533-46ba-aa8b-bfa8ccf96607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.chdir(\"/projectnb/labci/Lucia/edu-bayesian-MCMC/\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "from pymc import PolyaGamma as PG\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "from src.samplers.MetropolisGibbs import gibbs\n",
    "from src.samplers.PolyaGamma import polyagamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7fa6ab-4d60-4310-b163-cde69a56e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "I = 100   # exam items\n",
    "P = 1000  # pupils\n",
    "\n",
    "init_a = torch.zeros(I)  \n",
    "init_b = torch.zeros(I) \n",
    "init_t = torch.zeros(P) \n",
    "init_W = torch.zeros(P, I)\n",
    "\n",
    "\n",
    "true_a = torch.empty(size=(I,))      # items' discriminatory power\n",
    "true_b = torch.empty(size=(I,))      # items' difficulty\n",
    "true_theta = torch.empty(size=(P,))  # students' skills\n",
    "true_W = torch.empty(size=(P,I))     # latent variable (for PG case)\n",
    "Y = torch.empty(size=(P, I))         # simulated data\n",
    "\n",
    "# populate a, b:\n",
    "for i in range(I):\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    true_a[i] = U.sample()\n",
    "    true_b[i] = -1.0*U.sample()\n",
    "    \n",
    "# populate theta:\n",
    "for p in range(P):\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    true_theta[p] = U.sample()\n",
    "\n",
    "# populate W:\n",
    "scales = torch.mul(torch.t(true_a[:, None]), true_theta[:, None]) + true_b\n",
    "true_W = torch.from_numpy(pm.draw(PG.dist(h=1, z=scales)))\n",
    "\n",
    "# generate exam data:\n",
    "for i in range(I):\n",
    "    for p in range(P):\n",
    "        prob = torch.exp(true_a[i]*true_theta[p] + true_b[i]) / (1 + torch.exp(true_a[i]*true_theta[p] + true_b[i]))\n",
    "        B = torch.distributions.Bernoulli(prob)\n",
    "        Y[p][i] = B.sample()\n",
    "\n",
    "#Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63a1c29-c84d-4c74-ada3-f82429e7f6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs sampler... \n",
      "--------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9999 [00:41<58:16:43, 20.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      3\u001b[0m sigmasq_a, sigmasq_b, sigmasq_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])  \u001b[38;5;66;03m# priors\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m samples_mwg \u001b[38;5;241m=\u001b[39m gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter\u001b[38;5;241m=\u001b[39mN, adapt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m samples_ada_mwg \u001b[38;5;241m=\u001b[39m gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter\u001b[38;5;241m=\u001b[39mN, adapt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/projectnb/labci/Lucia/edu-bayesian-MCMC/src/samplers/MetropolisGibbs.py:281\u001b[0m, in \u001b[0;36mgibbs\u001b[0;34m(init_a, init_b, init_theta, y, sigmasq_a, sigmasq_b, sigmasq_t, niter, adapt)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(I):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapt:\n\u001b[0;32m--> 281\u001b[0m         B[s][i], ad \u001b[38;5;241m=\u001b[39m metropolis(B[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][i], sigmasq_b, avg_acc_b[i], logb_conditional, A[s][i], THETA[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y[:,i], sigmasq_b)\n\u001b[1;32m    282\u001b[0m         avg_acc_b[i] \u001b[38;5;241m=\u001b[39m avg_acc_b[i] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39ms)\u001b[38;5;241m*\u001b[39mad\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/projectnb/labci/Lucia/edu-bayesian-MCMC/src/samplers/MetropolisGibbs.py:139\u001b[0m, in \u001b[0;36mmetropolis\u001b[0;34m(prev_sample, sigmasq, avg_acc, full_cond, *full_cond_args)\u001b[0m\n\u001b[1;32m    131\u001b[0m prop \u001b[38;5;241m=\u001b[39m prev_sample \u001b[38;5;241m+\u001b[39m (torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m]), sigmasq)\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Ensure that values are within range [0,1]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#if (prop > 1) or (prop < 0):\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#    A = 0\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#    logr = full_cond(prop, *full_cond_args) - full_cond(prev_sample, *full_cond_args)    \u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#    A = torch.min(torch.tensor([1.0]), torch.exp(logr)) \u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m logr \u001b[38;5;241m=\u001b[39m full_cond(prop, \u001b[38;5;241m*\u001b[39mfull_cond_args) \u001b[38;5;241m-\u001b[39m full_cond(prev_sample, \u001b[38;5;241m*\u001b[39mfull_cond_args)    \n\u001b[1;32m    140\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mexp(logr)) \n\u001b[1;32m    142\u001b[0m U \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mUniform(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])))\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m/projectnb/labci/Lucia/edu-bayesian-MCMC/src/samplers/MetropolisGibbs.py:69\u001b[0m, in \u001b[0;36mlogb_conditional\u001b[0;34m(b, a, theta, y, sigmasq)\u001b[0m\n\u001b[1;32m     67\u001b[0m logsum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 69\u001b[0m     logsum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y[p] \u001b[38;5;241m*\u001b[39m b) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(a\u001b[38;5;241m*\u001b[39mtheta[p]\u001b[38;5;241m+\u001b[39mb))  \n\u001b[1;32m     71\u001b[0m prob \u001b[38;5;241m=\u001b[39m logsum \u001b[38;5;241m-\u001b[39m (torch\u001b[38;5;241m.\u001b[39mpow(b, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39msigmasq))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run samplers\n",
    "N = 10000\n",
    "sigmasq_a, sigmasq_b, sigmasq_t = torch.tensor([1.0]), torch.tensor([1.0]), torch.tensor([1.0])  # priors\n",
    "\n",
    "samples_mwg = gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N, adapt=False)\n",
    "print(\"\")\n",
    "samples_ada_mwg = gibbs(init_a, init_b, init_t, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N, adapt=True)\n",
    "print(\"\")\n",
    "samples_pg = polyagamma(init_a, init_b, init_t, init_W, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4539b3-bf3c-4862-8b1d-85ed4903c11f",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc46fd-14d2-4977-8d6a-3e5b33b0fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9), sharex=True)\n",
    "fig1.suptitle(\"Traceplots for MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "t = range(N)\n",
    "data_to_plot = [samples_mwg[0][:,0], samples_ada_mwg[0][:,0], samples_pg[0][:,0],\n",
    "                samples_mwg[1][:,0], samples_ada_mwg[1][:,0], samples_pg[1][:,0],\n",
    "                samples_mwg[2][:,0], samples_ada_mwg[2][:,0], samples_pg[2][:,0]]\n",
    "\n",
    "true_values = [true_a[0], true_a[0], true_a[0],\n",
    "               true_b[0], true_b[0], true_b[0],\n",
    "               true_theta[0], true_theta[0], true_theta[0]]\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig1.axes):\n",
    "    ax.plot(t, data_to_plot[i])\n",
    "    ax.axhline(true_values[i], color=\"red\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig1.supxlabel('iterations')\n",
    "fig1.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0612b8-4947-4db6-99c5-fe48ab2bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9))\n",
    "fig2.suptitle(\"Histograms of MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig2.axes):\n",
    "    ax.hist(data_to_plot[i], bins=100)\n",
    "    ax.axvline(true_values[i], color=\"red\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ed978-8b85-4a8f-b5a2-0a5720369c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9), sharey=True)\n",
    "fig3.suptitle(\"ACF for MCMC samples of a, b, theta under 3 sampling schemes\")\n",
    "\n",
    "cols = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a[0]\", \"b[0]\", \"theta[0]\"]\n",
    "\n",
    "for i, ax in enumerate(fig3.axes):\n",
    "    tsaplots.plot_acf(data_to_plot[i], lags=50, ax=ax, title=\"\")\n",
    "\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col, size=\"small\")\n",
    "\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "fig3.supxlabel('lag')\n",
    "fig3.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41f25b-c670-4c59-a385-d5eba173998c",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1df740-edc3-4a7f-9b5d-15f4cb028874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(chain, true_val):\n",
    "    MSE = nn.MSELoss()\n",
    "    E = torch.empty(size=(len(chain),))\n",
    "\n",
    "    for s in range(len(chain)):\n",
    "        E[s] = MSE(chain[s], true_val)\n",
    "\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87d0be-334c-43ee-9be7-0d2251e3da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, axs = plt.subplots(nrows=3, ncols=1, figsize=(9,9), sharex=True)\n",
    "fig4.suptitle(\"MSE of MCMC samples of a, b, theta under 3 sampling schemes over time\")\n",
    "\n",
    "t = range(N)\n",
    "errors_to_plot = [[mse(samples_mwg[0], true_a), mse(samples_ada_mwg[0], true_a), mse(samples_pg[0], true_a)],\n",
    "                  [mse(samples_mwg[1], true_b), mse(samples_ada_mwg[1], true_b), mse(samples_pg[1], true_b)],\n",
    "                  [mse(samples_mwg[2], true_theta), mse(samples_ada_mwg[2], true_theta), mse(samples_pg[2], true_theta)]]\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "labels = [\"Metropolis within Gibbs\", \"Adaptive MwG\", \"Gibbs (Polya-Gamma)\"]\n",
    "rows = [\"a\", \"b\", \"theta\"]\n",
    "\n",
    "for i, ax in enumerate(fig4.axes):\n",
    "    lines = data_to_plot[i]\n",
    "    \n",
    "    for j in range(len(lines)):\n",
    "        ax.plot(t, lines[j], color=colors[j], label=labels[j])\n",
    "\n",
    "for ax, row in zip(axs, rows):\n",
    "    ax.set_ylabel(row, rotation=90)\n",
    "\n",
    "axs[0].legend(loc=\"best\")\n",
    "fig4.supxlabel('iterations')\n",
    "fig4.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97e07e-5eb7-42bf-a142-4fbd413c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(errors_to_plot)):\n",
    "    for j in range(len(errors_to_plot[i])):\n",
    "        mean = torch.mean(errors_to_plot[i][j])\n",
    "        print(f\"Overall MSE for variable {rows[i]} under {labels[j]} scheme: {mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874cbd4-f779-48d8-be18-e2d9d49ee8f6",
   "metadata": {},
   "source": [
    "### Asymptotic Variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
