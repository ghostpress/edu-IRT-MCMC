{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed881fb-ab6a-4255-9bb1-e0a3476d7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor as pt\n",
    "from pymc import PolyaGamma as PG\n",
    "from pymc import Normal as N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212b7219-11f9-40e1-9b6e-5a1afeafe119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_mean_a(v, b, theta, w, y):\n",
    "    S = torch.sum(torch.mul(theta, y-0.5+torch.mul(b, w)))  \n",
    "    mean = v*S        \n",
    "    return mean\n",
    "\n",
    "def _compute_var_a(sigmasq, theta, w):\n",
    "    S = torch.sum(torch.mul(w, torch.pow(theta, 2))) + 1/sigmasq\n",
    "    var = 1/S\n",
    "    return var\n",
    "\n",
    "def _compute_mean_b(v, a, theta, w, y):\n",
    "    S = torch.sum(y-0.5-a*torch.mul(theta, w))\n",
    "    mean = v*S\n",
    "    return mean\n",
    "\n",
    "def _compute_var_b(sigmasq, w):\n",
    "    S = torch.sum(w) + 1/sigmasq\n",
    "    var = 1/S\n",
    "    return var\n",
    "\n",
    "def _compute_mean_t(v, a, b, w, y):\n",
    "    S = torch.sum(torch.mul(a, y-0.5+torch.mul(b, w)))  \n",
    "    mean = v*S\n",
    "    return mean    \n",
    "\n",
    "def _compute_var_t(sigmasq, a, w):\n",
    "    S = torch.sum(torch.mul(w, torch.pow(a, 2))) + 1/sigmasq\n",
    "    #print(\"S \", S)\n",
    "    #print(torch.mul(w, torch.pow(a, 2)))\n",
    "    var = 1/S\n",
    "    return var\n",
    "\n",
    "def polyagamma(init_a, init_b, init_t, init_w, y, sigmasq_a, sigmasq_b, sigmasq_t, niter=10000):\n",
    "\n",
    "    assert init_a.shape == init_b.shape\n",
    "    assert init_t.shape[0] == y.shape[0]\n",
    "    assert y.shape == init_w.shape\n",
    "    \n",
    "    I = len(init_a)\n",
    "    P = len(init_t)\n",
    "    \n",
    "    # samples\n",
    "    A = torch.empty(size=(niter, I))\n",
    "    B = torch.empty(size=(niter, I))\n",
    "    THETA = torch.empty(size=(niter, P))\n",
    "    W = torch.empty(size=(niter, P, I))\n",
    "    \n",
    "    A[0] = init_a\n",
    "    B[0] = init_b\n",
    "    THETA[0] = init_t\n",
    "    W[0] = init_w\n",
    "\n",
    "    print(f\"Starting Gibbs sampler for Polya-Gamma... \\n--------------------------------------------\\n\")\n",
    "    for s in trange(niter):\n",
    "\n",
    "        # sample Ws\n",
    "        for p in range(P):\n",
    "            for i in range(I):\n",
    "                W[s][p][i] = torch.from_numpy(pm.draw(PG.dist(h=1, z=(A[s-1][i]*THETA[s-1][p] - B[s-1][i]))))\n",
    "\n",
    "        # sample As\n",
    "        for i in range(I):\n",
    "            var_a = float(_compute_var_a(sigmasq_a, THETA[s-1], W[s][:,i]))\n",
    "            mean_a = float(_compute_mean_a(var_a, B[s-1][i], THETA[s-1], W[s][:,i], y[:,i]))\n",
    "            print(var_a)\n",
    "            print(mean_a)\n",
    "            #try:\n",
    "            A[s][i] = torch.distributions.Normal(mean_a, var_a).sample()\n",
    "            #except ValueError:\n",
    "            #    A[s][i] = torch.distributions.Normal(mean_a, var_a+0.001).sample()\n",
    "\n",
    "        # sample Bs\n",
    "        for i in range(I):\n",
    "            var_b = float(_compute_var_b(sigmasq_b, W[s][:,i]))\n",
    "            mean_b = float(_compute_mean_b(var_b, A[s][i], THETA[s-1], W[s][:,i], y[:,i]))\n",
    "            B[s][i] = torch.distributions.Normal(mean_b, var_b).sample()\n",
    "\n",
    "        # sample THETAs\n",
    "        for p in range(P):\n",
    "            var_t = float(_compute_var_t(sigmasq_t, W[s][p], A[s]))\n",
    "            mean_t = float(_compute_mean_t(var_t, A[s], B[s], W[s][p,:], y[p,:]))\n",
    "            #print(\"mean: \", mean_t)\n",
    "            #print(\"variance: \", var_t)\n",
    "            #THETA[s][p] = torch.from_numpy(pm.draw(N.dist(mean_t, var_t)))\n",
    "            THETA[s][p] = torch.distributions.Normal(mean_t, var_t).sample()\n",
    "\n",
    "    return A, B, THETA, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e384c75-f066-4c1b-adf8-7373c83c71b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 1., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 1., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1., 1., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 0., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 1., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 1., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 1., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "I = 10   # exam items\n",
    "P = 100  # pupils\n",
    "sigmasq_a, sigmasq_b, sigmasq_t = torch.tensor([1.0]), torch.tensor([1.0]), torch.tensor([1.0])  # priors\n",
    "\n",
    "init_a = torch.zeros(I)  \n",
    "init_b = torch.zeros(I)  \n",
    "init_t = torch.zeros(P)\n",
    "init_W = torch.zeros(P, I)\n",
    "\n",
    "true_a = torch.tensor([1, 0.9, 0.01, 0.5, 0.7, 0.4, 0.03, 0.9, 0.8, 1])  # items' discriminatory power\n",
    "true_b = torch.tensor([0.01, 0.9, 1, 0.8, 0.2, 0.3, 0.88, 1, 0.3, 0.5])  # items' difficulty\n",
    "true_theta = torch.empty(size=(P,)) #torch.tensor([1, 1, 0, 0.5, 0.7, 0.1, 0.3, 0.9, 0.6, 0.7])  # students' skills\n",
    "true_W = torch.empty(size=(P,I))\n",
    "\n",
    "# populate theta:\n",
    "for p in range(P):\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    true_theta[p] = U.sample()\n",
    "\n",
    "# populate W:\n",
    "for p in range(P):\n",
    "    for i in range(I):\n",
    "        scale = true_a[i]*true_theta[p] - true_b[i]\n",
    "        true_W[p][i] = torch.from_numpy(pm.draw(PG.dist(h=1, z=scale)))\n",
    "        \n",
    "# exam data:\n",
    "Y = torch.empty(size=(P, I))\n",
    "\n",
    "for i in range(I):\n",
    "    #print(i)\n",
    "    for p in range(P):\n",
    "        #print(p)\n",
    "        prob = torch.exp(true_a[i]*true_theta[p] - true_b[i]) / (1 + torch.exp(true_a[i]*true_theta[p] - true_b[i]))\n",
    "        B = torch.distributions.Bernoulli(prob)\n",
    "        Y[p][i] = B.sample()\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0d9edd-b528-476c-a6a0-e41d2679a591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs sampler for Polya-Gamma... \n",
      "--------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:06<10:35,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.1502503027281782e+30\n",
      "1.0\n",
      "-1.8488902076008173e-10\n",
      "1.0\n",
      "1.989342166010466e+20\n",
      "1.0\n",
      "1.7561342247063294e-07\n",
      "1.0\n",
      "-1.7618344827496912e-07\n",
      "1.0\n",
      "-1.762647485747948e-07\n",
      "1.0\n",
      "-1.7744716274137318e-07\n",
      "1.0\n",
      "-1.801071505269647e-07\n",
      "1.0\n",
      "-4.786611107476801e-10\n",
      "1.0\n",
      "1.7680243047379918e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:12<21:09, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape ()) of distribution Normal(loc: nan, scale: 0.0) to satisfy the constraint Real(), but found invalid values:\nnan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m polyagamma(init_a, init_b, init_t, init_W, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m, in \u001b[0;36mpolyagamma\u001b[0;34m(init_a, init_b, init_t, init_w, y, sigmasq_a, sigmasq_b, sigmasq_t, niter)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mean_a)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     A[s][i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mean_a, var_a)\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m#except ValueError:\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#    A[s][i] = torch.distributions.Normal(mean_a, var_a+0.001).sample()\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# sample Bs\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(I):\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/edu-mcmc/lib/python3.11/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "File \u001b[0;32m/projectnb/labci/luciav/.conda/envs/edu-mcmc/lib/python3.11/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape ()) of distribution Normal(loc: nan, scale: 0.0) to satisfy the constraint Real(), but found invalid values:\nnan"
     ]
    }
   ],
   "source": [
    "samples = polyagamma(init_a, init_b, init_t, init_W, Y, sigmasq_a, sigmasq_b, sigmasq_t, niter=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
