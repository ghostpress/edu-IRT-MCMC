{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13651abe",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Suppose that a group of $P$ students is given a test with $I$ multiple choice questions. Let $Y_{pi}=1$ mean that student $p \\in \\{1, ..., P\\}$ has the correct answer on item $i \\in \\{1, ..., I\\}$. Assuming that all responses are conditionally independent given the parameters $a, b, \\theta$, we have:\n",
    "\n",
    "$$Y_{pi} | a,b,\\theta \\sim Ber(\\frac{e^{a_i\\theta_p - b_i}}{1 + e^{a_i\\theta_p - b_i}})$$\n",
    "\n",
    "where $\\theta_p$ measures the student's learning ability, $a_i$ measures the items discriminatory power, and $b_i$ measures the item's difficulty. Take the following prior assumptions on the distribution of parameters $\\theta, a, b$, where all the priors are assumed independent: $a_i \\sim \\mathcal{N}(0, \\sigma_a^2), b_i \\sim \\mathcal{N}(0, \\sigma_b^2), \\theta_p \\sim \\mathcal{N}(0,1)$. Then the posterior distribution of $(a, b, \\theta)$ is given by\n",
    "\n",
    "$$\\pi(a,b,\\theta | y) = exp\\bigr\\{ \\frac{1}{2\\sigma_a^2}||a||^2 - \\frac{1}{2\\sigma_b^2}||b||^2 + \\frac{1}{2}||\\theta||^2 + \\sum_{p,i} y_{pi}(a_i\\theta_p - b_i) - log(1 + e^{a_i\\theta_p - b_i}) \\bigl\\}$$\n",
    "\n",
    "and the full conditionals (the conditional distribution of one variable given all others) are given by\n",
    "\n",
    "$$\\pi(a_i | b,\\theta,y) = exp\\bigl\\{ \\frac{a_i^2}{2\\sigma_a^2} + \\sum_{p=1}^P a_i y_{pi} \\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "\n",
    "$$\\pi(b_i | a, \\theta, y) = exp\\bigl\\{ \\frac{b_i^2}{2\\sigma_b^2} \\sum_{p=1}^P y_{pi}b_i - log(1+e^{a_i\\theta_p - b_i}) \\bigr \\}$$\n",
    "\n",
    "$$\\pi(\\theta_p | a, b, y) = exp\\bigl\\{ \\frac{\\theta_p^2}{2} + \\sum_{i=1}^I a_iy_{pi}\\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "Since these full conditionals are not easy to sample from, below is an implementation of the Metropolis-within-Gibbs sampler to make sampling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cfcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_conditional():  # outputs a 1xP row\n",
    "    pass\n",
    "\n",
    "def a_conditional():      # outputs a 1xI row\n",
    "    pass\n",
    "\n",
    "def b_conditional():      # outputs a 1xI row\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acceptance():\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(init_a, init_b, init_theta, y, niter=10000):\n",
    "    \n",
    "    assert length(init_a) == length(init_b)\n",
    "    \n",
    "    I = length(init_a)\n",
    "    P = length(init_theta)\n",
    "    \n",
    "    AS = torch.empty(size=(niter, I))\n",
    "    BS = torch.empty(size=niter, I)\n",
    "    THETAS = torch.empty(size=(niter, P))\n",
    "    \n",
    "    AS[0] = init_a\n",
    "    BS[0] = init_b\n",
    "    THETAS[0] = init_theta\n",
    "    \n",
    "    for s in range(1, niter):\n",
    "        \n",
    "        # Sample next potential a from the full conditional\n",
    "        a_s = a_conditional(BS[s-1], THETAS[s-1], y)\n",
    "        \n",
    "        # Compute acceptance probability & sample from uniform\n",
    "        A_a = torch.min(1, compute_acceptance())\n",
    "        U_a = torch.distributions.Uniform(torch.tensor[0.0], torch.tensor[1.0])\n",
    "        \n",
    "        # Keep sample with computed acceptance probability\n",
    "        if U-a <= A_a:\n",
    "            AS[s] = a_s\n",
    "        else:\n",
    "            AS[s] = AS[s-1]\n",
    "        \n",
    "        # Sample next potential b from the full conditional\n",
    "        b_s = b_conditional(AS[s], THETAS[s-1], y)\n",
    "        \n",
    "        # Compute acceptance probability & sample from uniform\n",
    "        A_b = torch.min(1, compute_acceptance())\n",
    "        U_b = torch.distributions.Uniform(torch.tensor[0.0], torch.tensor[1.0])\n",
    "        \n",
    "        # Keep sample with computed acceptance probability\n",
    "        if U_b <= A_b:\n",
    "            BS[s] = b_s\n",
    "        else:\n",
    "            BS[s] = BS[s-1]\n",
    "        \n",
    "        # Sample next potential theta from the full conditional\n",
    "        theta_s = theta_conditional(AS[s], BS[s], y)\n",
    "        \n",
    "        # Compute acceptance probability & sample from uniform\n",
    "        A_t = torch.min(1, compute_acceptance())\n",
    "        U_t = torch.distributions.Uniform(torch.tensor[0.0], torch.tensor[1.0])\n",
    "        \n",
    "        # Keep sample with computed acceptance probability\n",
    "        if U_t <= A_t:\n",
    "            THETAS[s] = theta_s\n",
    "        else:\n",
    "            THETAS[s] = THETAS[s-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  1. Implement full conditionals\n",
    "#  2. Implement acceptance probability\n",
    "#  3. How to modularize the sampling so not so much code is repeated?\n",
    "#  4. Test & debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
