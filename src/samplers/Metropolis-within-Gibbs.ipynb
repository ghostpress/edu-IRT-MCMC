{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13651abe",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Suppose that a group of $P$ students is given a test with $I$ multiple choice questions. Let $Y_{pi}=1$ mean that student $p \\in \\{1, ..., P\\}$ has the correct answer on item $i \\in \\{1, ..., I\\}$. Assuming that all responses are conditionally independent given the parameters $a, b, \\theta$, we model the outcomes with:\n",
    "\n",
    "$$Y_{pi} | a,b,\\theta \\sim Ber(\\frac{e^{a_i\\theta_p - b_i}}{1 + e^{a_i\\theta_p - b_i}})$$\n",
    "\n",
    "where $\\theta_p$ measures the student's learning ability, $a_i$ measures the items discriminatory power, and $b_i$ measures the item's difficulty. Take the following prior assumptions on the distribution of parameters $\\theta, a, b$, where all the priors are assumed independent: $a_i \\sim \\mathcal{N}(0, \\sigma_a^2), b_i \\sim \\mathcal{N}(0, \\sigma_b^2), \\theta_p \\sim \\mathcal{N}(0,1)$. Then the posterior distribution of $(a, b, \\theta)$ is given by\n",
    "\n",
    "$$\\pi(a,b,\\theta | y) = exp\\bigr\\{ \\frac{1}{2\\sigma_a^2}||a||^2 - \\frac{1}{2\\sigma_b^2}||b||^2 + \\frac{1}{2}||\\theta||^2 + \\sum_{p,i} y_{pi}(a_i\\theta_p - b_i) - log(1 + e^{a_i\\theta_p - b_i}) \\bigl\\}$$\n",
    "\n",
    "and the full conditionals (the conditional distribution of one variable given all others) are given by\n",
    "\n",
    "$$\\pi(a_i | b,\\theta,y) = exp\\bigl\\{ \\frac{a_i^2}{2\\sigma_a^2} + \\sum_{p=1}^P a_i y_{pi} \\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "\n",
    "$$\\pi(b_i | a, \\theta, y) = exp\\bigl\\{ \\frac{b_i^2}{2\\sigma_b^2} + \\sum_{p=1}^P y_{pi}b_i - log(1+e^{a_i\\theta_p - b_i}) \\bigr \\}$$\n",
    "\n",
    "$$\\pi(\\theta_p | a, b, y) = exp\\bigl\\{ \\frac{\\theta_p^2}{2} + \\sum_{i=1}^I a_iy_{pi}\\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "Since these full conditionals are not easy to sample from, below is an implementation of the Metropolis-within-Gibbs sampler to make sampling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cfcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0863c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loga_conditional(a, b, theta, y, sigma):\n",
    "    \"\"\"Function to compute the (log) conditional probability of an observed value of \n",
    "    variable a at a given index i. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        The observed value of a at index i\n",
    "    b : torch.tensor\n",
    "        The observed value of b at index i\n",
    "    theta : torch.tensor\n",
    "        A vector of observed theta values\n",
    "    y : torch.tensor\n",
    "        The ith column vector of the data Y\n",
    "    sigma : float\n",
    "        The variance of variable a\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert theta.shape[0] == y.shape[0]\n",
    "    \n",
    "    logsum = 0\n",
    "    for p in range(y.shape[0]):\n",
    "        logsum += a * y[p] * theta[p] - torch.log(1 + torch.exp(a*theta[p]-b))\n",
    "    \n",
    "    return (torch.pow(a, 2)/(2*sigma**2)) + logsum\n",
    "\n",
    "def logb_conditional(a, b, theta, y, sigma):\n",
    "    \"\"\"Function to compute the (log) conditional probability of an observed value of \n",
    "    variable b at a given index i. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        The observed value of a at index i\n",
    "    b : torch.tensor\n",
    "        The observed value of b at index i\n",
    "    theta : torch.tensor\n",
    "        A vector of observed theta values\n",
    "    y : torch.tensor\n",
    "        The ith column vector of the data Y\n",
    "    sigma : float\n",
    "        The variance of variable a\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert theta.shape[0] == y.shape[0]\n",
    "    \n",
    "    logsum = 0\n",
    "    for p in range(y.shape[0]):\n",
    "        logsum += b * y[p] - torch.log(1 + torch.exp(a*theta[p]-b))\n",
    "    \n",
    "    return (torch.pow(b, 2)/(2*sigma**2)) + logsum\n",
    "\n",
    "def logtheta_conditional(a, b, theta, y):\n",
    "    \"\"\"Function to compute the (log) conditional probability of an observed value of \n",
    "    variable theta at a given index p. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        A vector of observed a values\n",
    "    b : torch.tensor\n",
    "        A vector of observed b values\n",
    "    theta : torch.tensor\n",
    "        The observed value of theta at index p\n",
    "    y : torch.tensor\n",
    "        The pth row vector of the data Y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert a.shape == b.shape\n",
    "    \n",
    "    logsum = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        logsum += a[i] * y[i] * theta - torch.log(1 + torch.exp(a[i]*theta - b[i]))\n",
    "    \n",
    "    return (torch.pow(theta, 2)/2) + logsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815c11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(prev_sample, sigma, full_cond, *full_cond_args):\n",
    "    \n",
    "    prop = prev_sample + torch.distributions.MultivariateNormal(torch.zeros(len(prev_sample)), torch.eye(sigma))\n",
    "        \n",
    "    logr = full_cond(prop, *full_cond_args) - full_cond(prev_sample, *full_cond_args)  # TODO: *?\n",
    "    A = torch.min(1, torch.exp(logr))\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "        \n",
    "    if U <= A:\n",
    "        samples.append(prop)\n",
    "    else:\n",
    "        samples.append(prev_sample)\n",
    "              \n",
    "    print(f\"Done sampling from {full_cond}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc1d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(init_a, init_b, init_theta, y, sigma_a, sigma_b, sigma_t, niter=10000):\n",
    "    \n",
    "    assert init_a.shape == init_b.shape\n",
    "    assert init_theta.shape[1] == y.shape[0]\n",
    "    \n",
    "    I = len(init_a)\n",
    "    P = len(init_theta)\n",
    "    \n",
    "    A = torch.empty(size=(niter, I))\n",
    "    B = torch.empty(size=(niter, I))\n",
    "    THETA = torch.empty(size=(niter, P))\n",
    "    \n",
    "    A[0] = init_a\n",
    "    B[0] = init_b\n",
    "    THETA[0] = init_theta\n",
    "    \n",
    "    print(f\"Starting Gibbs sampler... \\n--------------------------------------------\\n\")\n",
    "    for s in trange(1, niter):\n",
    "        \n",
    "        A[s] = metropolis(A[s-1], sigma_a, loga_conditional, A[s-1], B[s-1], THETA[s-1], y, sigma_a)\n",
    "        B[s] = metropolis(B[s-1], sigma_b, logb_conditional, B[s-1], A[s], THETA[s-1], y, sigma_b)\n",
    "        THETA[s] = metropolis(THETA[s-1], sigma_t, logtheta_conditional, THETA[s], A[s], B[s], y, sigma_t)\n",
    "    \n",
    "        if s % 100 == 0:\n",
    "            print(f\"Current samples at iteration {s}: \\nA\\n{A[s]} \\nB\\n{B[s]} \\nTHETA\\n{THETA[s]}\")\n",
    "            print(\"\\n--------------------------------------------\\n\")\n",
    "    \n",
    "    print(\"Done sampling.\")\n",
    "    return A, B, THETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95779801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 0., 1., 0., 0., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "I = 10  # exam items\n",
    "P = 10  # pupils\n",
    "sigma_a, sigma_b, sigma_t = 100.0, 100.0, 1.0\n",
    "\n",
    "init_a = torch.distributions.MultivariateNormal(torch.zeros(I), sigma_a*torch.eye(I)).sample()\n",
    "init_b = torch.distributions.MultivariateNormal(torch.zeros(I), sigma_b*torch.eye(I)).sample()\n",
    "init_t = torch.distributions.MultivariateNormal(torch.zeros(P), sigma_t*torch.eye(P)).sample()\n",
    "\n",
    "true_a = torch.tensor([1, 0.9, 0.01, 0.5, 0.7, 0.4, 0.03, 0.9, 0.8, 1])  # items' discriminatory power\n",
    "true_b = torch.tensor([0.01, 0.9, 1, 0.8, 0.2, 0.3, 0.88, 1, 0.3, 0.5])  # items' difficulty\n",
    "true_theta = torch.tensor([1, 1, 0, 0.5, 0.7, 0.1, 0.3, 0.9, 0.6, 0.7])  # students' skills\n",
    "\n",
    "# exam data:\n",
    "Y = torch.empty(size=(P, I))\n",
    "for i in range(P):\n",
    "    for j in range(I):\n",
    "        p = torch.exp(true_a[i]*true_theta[j] - true_b[i]) / (1 + torch.exp(true_a[i]*true_theta[j] - true_b[i]))\n",
    "        B = torch.distributions.Bernoulli(p)\n",
    "        Y[i, j] = B.sample()\n",
    "        \n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20a4b709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.5201)\n",
      "tensor(14.4100)\n",
      "tensor([-1.8374, -1.3133,  1.3199, -0.7984,  0.4708, -1.8918,  0.2370,  0.7092,\n",
      "        -1.1899, -0.2501])\n",
      "tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(init_a[0])\n",
    "print(init_b[0])\n",
    "print(init_t)\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c1221f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(init_t.shape[0])\n",
    "print(Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6a85f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-19.6841)\n",
      "tensor(-20.2758)\n",
      "tensor(-48.0334)\n"
     ]
    }
   ],
   "source": [
    "print(loga_conditional(a=init_a[0], b=init_b[0], theta=init_t, y=Y[:, 0], sigma=sigma_a))\n",
    "print(logb_conditional(a=init_a[0], b=init_b[0], theta=init_t, y=Y[:, 0], sigma=sigma_b))\n",
    "print(logtheta_conditional(a=init_a, b=init_b, theta=theta[0], y=Y[0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883beeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  1. Implement Gibbs as if full-conditionals could be sampled from -> test w/ normal FCs\n",
    "#  2. Sub-implement MH to sample from FCs -> test w/ normal FCs\n",
    "#  3. Test MH-within-Gibbs sampler with actual FCs\n",
    "#  4. Check q,p proposal and target distributions from notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
