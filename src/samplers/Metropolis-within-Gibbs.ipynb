{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13651abe",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Suppose that a group of $P$ students is given a test with $I$ multiple choice questions. Let $Y_{pi}=1$ mean that student $p \\in \\{1, ..., P\\}$ has the correct answer on item $i \\in \\{1, ..., I\\}$. Assuming that all responses are conditionally independent given the parameters $a, b, \\theta$, we model the outcomes with:\n",
    "\n",
    "$$Y_{pi} | a,b,\\theta \\sim Ber(\\frac{e^{a_i\\theta_p - b_i}}{1 + e^{a_i\\theta_p - b_i}})$$\n",
    "\n",
    "where $\\theta_p$ measures the student's learning ability, $a_i$ measures the items discriminatory power, and $b_i$ measures the item's difficulty. Take the following prior assumptions on the distribution of parameters $\\theta, a, b$, where all the priors are assumed independent: $a_i \\sim \\mathcal{N}(0, \\sigma_a^2), b_i \\sim \\mathcal{N}(0, \\sigma_b^2), \\theta_p \\sim \\mathcal{N}(0,1)$. Then the posterior distribution of $(a, b, \\theta)$ is given by\n",
    "\n",
    "$$\\pi(a,b,\\theta | y) = exp\\bigr\\{ \\frac{1}{2\\sigma_a^2}||a||^2 - \\frac{1}{2\\sigma_b^2}||b||^2 + \\frac{1}{2}||\\theta||^2 + \\sum_{p,i} y_{pi}(a_i\\theta_p - b_i) - log(1 + e^{a_i\\theta_p - b_i}) \\bigl\\}$$\n",
    "\n",
    "and the full conditionals (the conditional distribution of one variable given all others) are given by\n",
    "\n",
    "$$\\pi(a_i | b,\\theta,y) = exp\\bigl\\{ -\\frac{a_i^2}{2\\sigma_a^2} + \\sum_{p=1}^P a_i y_{pi} \\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "\n",
    "$$\\pi(b_i | a, \\theta, y) = exp\\bigl\\{ -\\frac{b_i^2}{2\\sigma_b^2} + \\sum_{p=1}^P y_{pi}b_i - log(1+e^{a_i\\theta_p - b_i}) \\bigr \\}$$\n",
    "\n",
    "$$\\pi(\\theta_p | a, b, y) = exp\\bigl\\{ \\frac{\\theta_p^2}{2} + \\sum_{i=1}^I a_iy_{pi}\\theta_p - log(1+e^{a_i\\theta_p - b_i}) \\bigr\\}$$\n",
    "\n",
    "Since these full conditionals are not easy to sample from, below is an implementation of the Metropolis-within-Gibbs sampler to make sampling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cfcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0863c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: a, b, theta are all vectors and Y is full matrix? ie no rows/columns?\n",
    "\n",
    "def loga_conditional(a, b, theta, y, sigma):\n",
    "    \"\"\"Function to compute the (log) full conditional probability of an observed value of \n",
    "    variable a at a given index i. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        The observed value of a at index i\n",
    "    b : torch.tensor\n",
    "        The observed value of b at index i\n",
    "    theta : torch.tensor\n",
    "        A vector of observed theta values\n",
    "    y : torch.tensor\n",
    "        The ith column vector of the data Y\n",
    "    sigma : float\n",
    "        The variance of variable a\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob : torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert theta.shape[0] == y.shape[0]\n",
    "    \n",
    "    logsum = 0\n",
    "    for p in range(y.shape[0]):\n",
    "        logsum += a * y[p] * theta[p] - torch.log(1 + torch.exp(a*theta[p]-b))\n",
    "    \n",
    "    prob = logsum - (torch.pow(a, 2)/(2*sigma**2))\n",
    "    return prob\n",
    "\n",
    "def logb_conditional(a, b, theta, y, sigma):\n",
    "    \"\"\"Function to compute the (log) full conditional probability of an observed value of \n",
    "    variable b at a given index i. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        The observed value of a at index i\n",
    "    b : torch.tensor\n",
    "        The observed value of b at index i\n",
    "    theta : torch.tensor\n",
    "        A vector of observed theta values\n",
    "    y : torch.tensor\n",
    "        The ith column vector of the data Y\n",
    "    sigma : float\n",
    "        The variance of variable a\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob : torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert theta.shape[0] == y.shape[0]\n",
    "    \n",
    "    logsum = 0\n",
    "    for p in range(y.shape[0]):\n",
    "        logsum += b * y[p] - torch.log(1 + torch.exp(a*theta[p]-b))\n",
    "    \n",
    "    prob = logsum - (torch.pow(b, 2)/(2*sigma**2))\n",
    "    return prob\n",
    "\n",
    "def logtheta_conditional(a, b, theta, y):\n",
    "    \"\"\"Function to compute the (log) full conditional probability of an observed value of \n",
    "    variable theta at a given index p. To get the actual conditional probability, raise\n",
    "    Euler's constant to the power of the returned value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : torch.tensor\n",
    "        A vector of observed a values\n",
    "    b : torch.tensor\n",
    "        A vector of observed b values\n",
    "    theta : torch.tensor\n",
    "        The observed value of theta at index p\n",
    "    y : torch.tensor\n",
    "        The pth row vector of the data Y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob : torch.tensor\n",
    "        The (log) conditional probability\n",
    "    \"\"\"\n",
    "    \n",
    "    assert a.shape == b.shape\n",
    "    \n",
    "    logsum = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        logsum += a[i] * y[i] * theta - torch.log(1 + torch.exp(a[i]*theta - b[i]))\n",
    "    \n",
    "    prob = logsum - (torch.pow(theta, 2)/2)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "815c11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(prev_sample, sigma, full_cond, *full_cond_args):\n",
    "    \n",
    "    n = len(prev_sample)\n",
    "    prop = prev_sample + (torch.distributions.MultivariateNormal(torch.zeros(n), sigma*torch.eye(n))).sample()\n",
    "        \n",
    "    logr = full_cond(prop, *full_cond_args) - full_cond(prev_sample, *full_cond_args)\n",
    "    print(*full_cond_args)\n",
    "    print(logr)\n",
    "    A = torch.min(1, torch.exp(logr))\n",
    "    U = torch.distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "        \n",
    "    if U <= A:\n",
    "        samples.append(prop)\n",
    "    else:\n",
    "        samples.append(prev_sample)\n",
    "              \n",
    "    print(f\"Done sampling from {full_cond}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfc1d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(init_a, init_b, init_theta, y, sigma_a, sigma_b, sigma_t, niter=10000):\n",
    "    \n",
    "    assert init_a.shape == init_b.shape\n",
    "    assert init_theta.shape[0] == y.shape[0]\n",
    "    \n",
    "    I = len(init_a)\n",
    "    P = len(init_theta)\n",
    "    \n",
    "    A = torch.empty(size=(niter, I))\n",
    "    B = torch.empty(size=(niter, I))\n",
    "    THETA = torch.empty(size=(niter, P))\n",
    "    \n",
    "    A[0] = init_a\n",
    "    B[0] = init_b\n",
    "    THETA[0] = init_theta\n",
    "    \n",
    "    print(f\"Starting Gibbs sampler... \\n--------------------------------------------\\n\")\n",
    "    for s in trange(1, niter):\n",
    "        \n",
    "        # FIXME: need ith column of Y\n",
    "        A[s] = metropolis(A[s-1], sigma_a, loga_conditional, B[s-1], THETA[s-1], y, sigma_a)\n",
    "        B[s] = metropolis(B[s-1], sigma_b, logb_conditional, A[s], THETA[s-1], y, sigma_b)\n",
    "        \n",
    "        # FIXME: need pth row of Y\n",
    "        THETA[s] = metropolis(THETA[s-1], sigma_t, logtheta_conditional, A[s], B[s], y, sigma_t)\n",
    "    \n",
    "        if s % 100 == 0:\n",
    "            print(f\"Current samples at iteration {s}: \\nA\\n{A[s]} \\nB\\n{B[s]} \\nTHETA\\n{THETA[s]}\")\n",
    "            print(\"\\n--------------------------------------------\\n\")\n",
    "    \n",
    "    print(\"Done sampling.\")\n",
    "    return A, B, THETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95779801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "I = 10  # exam items\n",
    "P = 10  # pupils\n",
    "sigma_a, sigma_b, sigma_t = 100.0, 100.0, 1.0\n",
    "\n",
    "init_a = torch.zeros(I)  #torch.distributions.MultivariateNormal(torch.zeros(I), sigma_a*torch.eye(I)).sample()\n",
    "init_b = torch.zeros(I)  #torch.distributions.MultivariateNormal(torch.zeros(I), sigma_b*torch.eye(I)).sample()\n",
    "init_t = torch.zeros(P)  #torch.distributions.MultivariateNormal(torch.zeros(P), sigma_t*torch.eye(P)).sample()\n",
    "\n",
    "true_a = torch.tensor([1, 0.9, 0.01, 0.5, 0.7, 0.4, 0.03, 0.9, 0.8, 1])  # items' discriminatory power\n",
    "true_b = torch.tensor([0.01, 0.9, 1, 0.8, 0.2, 0.3, 0.88, 1, 0.3, 0.5])  # items' difficulty\n",
    "true_theta = torch.tensor([1, 1, 0, 0.5, 0.7, 0.1, 0.3, 0.9, 0.6, 0.7])  # students' skills\n",
    "\n",
    "# exam data:\n",
    "Y = torch.empty(size=(P, I))\n",
    "for i in range(P):\n",
    "    for j in range(I):\n",
    "        p = torch.exp(true_a[i]*true_theta[j] - true_b[i]) / (1 + torch.exp(true_a[i]*true_theta[j] - true_b[i]))\n",
    "        B = torch.distributions.Bernoulli(p)\n",
    "        Y[i, j] = B.sample()\n",
    "        \n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "254b59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gibbs sampler... \n",
      "--------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([[1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]]) 100.0\n",
      "tensor([-5.2123e-03, -2.2268e-04, -2.7895e-04, -2.9516e-04, -1.3366e-03,\n",
      "        -4.8618e-03, -7.6294e-05, -8.1921e-04, -7.8583e-04, -1.2173e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "min() received an invalid combination of arguments - got (int, Tensor), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgibbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 20\u001b[0m, in \u001b[0;36mgibbs\u001b[0;34m(init_a, init_b, init_theta, y, sigma_a, sigma_b, sigma_t, niter)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Gibbs sampler... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m1\u001b[39m, niter):\n\u001b[0;32m---> 20\u001b[0m     A[s] \u001b[38;5;241m=\u001b[39m \u001b[43mmetropolis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloga_conditional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTHETA\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     B[s] \u001b[38;5;241m=\u001b[39m metropolis(B[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], sigma_b, logb_conditional, A[s], THETA[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y, sigma_b)\n\u001b[1;32m     22\u001b[0m     THETA[s] \u001b[38;5;241m=\u001b[39m metropolis(THETA[s\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], sigma_t, logtheta_conditional, A[s], B[s], y, sigma_t)\n",
      "Cell \u001b[0;32mIn[100], line 9\u001b[0m, in \u001b[0;36mmetropolis\u001b[0;34m(prev_sample, sigma, full_cond, *full_cond_args)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39mfull_cond_args)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(logr)\n\u001b[0;32m----> 9\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m U \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mUniform(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.0\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m]))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m A:\n",
      "\u001b[0;31mTypeError\u001b[0m: min() received an invalid combination of arguments - got (int, Tensor), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "gibbs(init_a, init_b, init_t, Y, sigma_a, sigma_b, sigma_t, niter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb6f3db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883beeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#  1. Implement Gibbs as if full-conditionals could be sampled from -> test w/ normal FCs\n",
    "#  2. Sub-implement MH to sample from FCs -> test w/ normal FCs\n",
    "#  3. Test MH-within-Gibbs sampler with actual FCs\n",
    "#  4. Check q,p proposal and target distributions from notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
